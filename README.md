# LUQ: Long-text Uncertainty Quantification for LLMs

This repo is for the paper [LUQ: Long-text Uncertainty Quantification for LLMs](https://arxiv.org/abs/2403.20279). An early version has been provided, and the complete code will be available soon.

**Update:** We have recently included the more advanced Llama3-8b-instruct as our NLI tool. By utilizing VLLM, we can significantly increase the speed of inference and achieve better performance. 
